{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0w/Q4oysDR9IYqFbbGZjV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Collinsngenokip/basic-ml-course/blob/Week5/05_Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, we will implement Bernoulli Naive Bayes and Multinomial Naive Bayes, and apply them for text classification.  We will experiment on the 20 newsgroups text dataset. It comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation).\n",
        "\n",
        "First, we load the dataset from sklearn."
      ],
      "metadata": {
        "id": "829Foe-zr4eQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Fx902QKhrFig"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can access to text by data property. For labels, their name and corresponding numeric values are stored in target_names and target. \\ Let's take a look at our data"
      ],
      "metadata": {
        "id": "-1-VywaHsLYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(newsgroups_train.data)"
      ],
      "metadata": {
        "id": "preDC9S8shmw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0150234-c624-45b9-90ff-73008d20f6ea"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_train.target, newsgroups_train.target_names"
      ],
      "metadata": {
        "id": "kIbYZhqFsoIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63ca31cd-ea5b-4dae-c2a3-1d7f41e1a6ad"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([7, 4, 4, ..., 3, 1, 8]),\n",
              " ['alt.atheism',\n",
              "  'comp.graphics',\n",
              "  'comp.os.ms-windows.misc',\n",
              "  'comp.sys.ibm.pc.hardware',\n",
              "  'comp.sys.mac.hardware',\n",
              "  'comp.windows.x',\n",
              "  'misc.forsale',\n",
              "  'rec.autos',\n",
              "  'rec.motorcycles',\n",
              "  'rec.sport.baseball',\n",
              "  'rec.sport.hockey',\n",
              "  'sci.crypt',\n",
              "  'sci.electronics',\n",
              "  'sci.med',\n",
              "  'sci.space',\n",
              "  'soc.religion.christian',\n",
              "  'talk.politics.guns',\n",
              "  'talk.politics.mideast',\n",
              "  'talk.politics.misc',\n",
              "  'talk.religion.misc'])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_train.data[0]"
      ],
      "metadata": {
        "id": "Ko-OAU6ZsuZV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "3ea1a9ec-b980-4993-a83b-36f330b649df"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When applying machine learning to solve problems, designing algorithm is not the only way to optimize. We can also intervene on data, i.e. data preprocessing, feature selection, etc. In some case, this approach is even better than model optimizing. For this dataset, you can notice that the text have lots of redundant information, for example punctuation, title, etc. We can remove those from our data to get better performance. Here I define a function to remove all punctuation from text."
      ],
      "metadata": {
        "id": "MPYEBwCYsxd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_tokens(token_list, text):\n",
        "    for token in token_list:\n",
        "        text = text.replace(token, '')\n",
        "    return text"
      ],
      "metadata": {
        "id": "sOeYiUqks18N"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "preprocessed_text = [remove_tokens(punctuation, text) for text in newsgroups_train.data]"
      ],
      "metadata": {
        "id": "J2cZWI6ls5dj"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_train.data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "jbwB1tHQNr6v",
        "outputId": "f0faa9e2-ab14-4a22-b0d0-c17f28a04f71"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 1** : First we have to transform text into numeric feature. You have to build a matrix that counts word occurences in each documents (0.5pt). For fast computing, we only select 30000 words with highest frequency. Hint You should use sklearn.feature_extraction.text.CountVectorizer and max_features argument."
      ],
      "metadata": {
        "id": "i9ZYs6ECs8t_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "num_word = 30000\n",
        "vectorizer = CountVectorizer(max_features=num_word)\n",
        "vectorizer.fit(preprocessed_text)\n",
        "\n",
        "train_data = vectorizer.transform(preprocessed_text).toarray()\n",
        "print(train_data.shape)"
      ],
      "metadata": {
        "id": "lkFZHquJs-UI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6510479d-7567-4e1d-81fb-8708e1737903"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11314, 30000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that for Naive Bayes, we find label that satisfy \n",
        "\n",
        "\n",
        "**Assigment 2** : We will derive prior probabilities  from data by computing frequency of class. You have to compute the number of documents in each class in class_freq variable, and divide to the total number of documents to get prior probability in prior_prob variable (1pt)"
      ],
      "metadata": {
        "id": "x6CpDJD7tE4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "classes,class_freq = np.unique(newsgroups_train.target, return_counts=True)\n",
        "prior_prob = class_freq/np.sum(class_freq)\n",
        "np.sum(prior_prob)\n",
        "print(class_freq)"
      ],
      "metadata": {
        "id": "x6mndFgZtaFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec4b575-b085-4b86-ab99-f758a94c8efb"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[480 584 591 590 578 593 585 594 598 597 600 595 591 594 593 599 546 564\n",
            " 465 377]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assigment 3** : In this step, we will implement Bernoulli Naive Bayes. Therefore, the conditional probability is probability of that a document with label  has the word . \\ To do that, we need the number of documents which has word  and label  for every pair . Your task is computing these values and store them in word_label_freq variable. It should be a numpy array for fast computing in the next step. (0.5.pt) \\ Hint: Our 'train_data features are the number of occurences of words in documents. You can convert them to binary feature that whether a word appears in a document."
      ],
      "metadata": {
        "id": "lopJBFF8ueM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_label_frequency = np.zeros((len(newsgroups_train.target_names),train_data.shape[1]))\n",
        "count = 0\n",
        "for i,b in enumerate(train_data[0]):\n",
        "  if b:\n",
        "    count += 1\n",
        "#print(count)  \n",
        "for i in range(len(newsgroups_train.target)): \n",
        "  word_label_frequency[newsgroups_train.target[i]] += train_data[i].clip(max=1)\n",
        "print(word_label_frequency) "
      ],
      "metadata": {
        "id": "VEs1W8Uhusrx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d13b613-f74d-4538-e3d6-bd8f2b97a587"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.  0.  0. ...  0.  0.  0.]\n",
            " [ 5.  0.  0. ... 10.  2.  1.]\n",
            " [ 2.  0.  1. ...  0.  0.  0.]\n",
            " ...\n",
            " [ 0.  4.  1. ...  0.  0.  0.]\n",
            " [ 1.  0.  0. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  0.  0.  0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assigment 4 **: The conditional probability is computed by dividing the number of documents which has word  and label  to the number of documents with label . However, if there is no document which has word  and label  in training data, the probability will be zero, which is undesirable. \\ To handle this problem, we can apply Laplace smoothing, then conditional probability will be computed as following\n",
        "\n",
        "Your task here is implementing this formula with default alpha=0.1 and then fill in all the probability values in cond_prob variable. It should be a numpy array for fast computing in the next step. (1pt)"
      ],
      "metadata": {
        "id": "zWTSR7qKuwpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.01"
      ],
      "metadata": {
        "id": "qopszBq4XwYP"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cond_prob = np.array([(word_label_frequency[i] + alpha)/(class_freq[i] + num_word * alpha) for i in range(len(class_freq))])\n",
        "print(cond_prob.shape)"
      ],
      "metadata": {
        "id": "dNJ8l6_Ju85R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe38f86-86cb-49c5-9c61-17d54066bc5e"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20, 30000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assigment 5** : For test data, the conditional probabily follows Bernoulli distribution and is computed by\n",
        "\n",
        "Then we multiply with prior probability and select the class with highest value as predicted label. For numerical stability, you should use log probability to compute. (2pt\n",
        "\n",
        "*Hint* Remember to convert test data feature to binary feature as training data."
      ],
      "metadata": {
        "id": "gGtN86wYvEE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_label(data):\n",
        "  result_class = 0\n",
        "  for i,class_val in enumerate(cond_prob):\n",
        "    prod = np.log(prior_prob[i]) + np.sum(np.log(class_val)) + np.sum(np.log(1 - class_val))\n",
        "  return result_class"
      ],
      "metadata": {
        "id": "ju0e5A1YvSTg"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can obtain labels and accuracy score of model on test data"
      ],
      "metadata": {
        "id": "wQagbiEOvWFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_test_text = [remove_tokens(punctuation, text) for text in newsgroups_test.data]\n",
        "test_data = vectorizer.transform(preprocessed_test_text)"
      ],
      "metadata": {
        "id": "Gch58au6vXSn"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "from tqdm import tqdm\n",
        "for text in tqdm(test_data):\n",
        "    pred.append(find_label(text))"
      ],
      "metadata": {
        "id": "GZywdWw3vc-m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "1db1585c-bf13-4920-dae8-07912954ea60"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1676it [00:41, 39.98it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-b53841df5d92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-65-51f2aa24f9ab>\u001b[0m in \u001b[0;36mfind_label\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mresult_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mclass_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "metrics.accuracy_score(pred, newsgroups_test.target)"
      ],
      "metadata": {
        "id": "SgtjSi3Qvgil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assigment 6** : Next, we will implement Multinomial Naive Bayes. For this model, the conditional probability follows multinomial distribution. We have to estimate conditional probabilities from data as the frequency of words in all documents with a given class. \\ First, you have to count the number of occurences of a word  in all document with class  (0.5pt)."
      ],
      "metadata": {
        "id": "RrZbvvbQvjX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_label_freq = ..."
      ],
      "metadata": {
        "id": "6sJq8xJevu4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assigment 7** : Your task here is to count the total number of words in all documents of class  to compute probability in next step. (0.5)pt \\ Hint You can sum the number of occurences of all words in class  that we obtained in last step."
      ],
      "metadata": {
        "id": "iIx4DnJzvv3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_word_in_classes = ..."
      ],
      "metadata": {
        "id": "O1ASAipgv5JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assigment 8 : Now we can compute conditional probability for every pair . Similar to Bernoulli Naive Bayes, we also add Laplace smoothing to avoid zero probability\n",
        "\n",
        "where  is the number of occurences of word  in all documents with class ,  is the total number of words in all documents of class . \\ For numerical stability, you should compute log of probablity (1pt)\n",
        "\n"
      ],
      "metadata": {
        "id": "YfyihZrPv9nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_cond_prob = ..."
      ],
      "metadata": {
        "id": "4pBT6Q5zwJMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 9 : After getting all necessary probabilities, we can find label of new test data. For this task, you have to implement find_label function that compute product of prior and conditional probablities, and select the label with highest value. Finally, you can get prediction for all test data and get accuracy score (3pt)."
      ],
      "metadata": {
        "id": "zoJvXy2uwMa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_label(data):\n",
        "    \"\"\"\n",
        "      Your code here\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "redyCz8lwQLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "for text in tqdm(test_data):\n",
        "    pred.append(find_label(text))\n",
        "metrics.accuracy_score(pred, newsgroups_test.target)"
      ],
      "metadata": {
        "id": "OJmrdMl0wTiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Optional) Try to improve performance of Naive Bayes model in this dataset. You can try everything to do this, i.e. change hyperparameters, preprocess data, ..."
      ],
      "metadata": {
        "id": "dPA5HTHiwXFq"
      }
    }
  ]
}